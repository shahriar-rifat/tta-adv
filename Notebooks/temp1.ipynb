{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Registry(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Registry,self).__init__(*agrs, **kwargs)\n",
    "\n",
    "    def register(self, module_name, module=None):\n",
    "        if module is not None:\n",
    "            _register_generic(self, module_name, module)\n",
    "            return\n",
    "        def register_fn(fn):\n",
    "            _register_generic(self, module_name, fn)\n",
    "            return fn\n",
    "        return register_fn\n",
    "    \n",
    "def register_generic(module_dict, module_name, module):\n",
    "    assert module_name not in module_dict\n",
    "    module_dict[module_name] = module \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# registry class extends the dictionary data structure and provide and \n",
    "# extra register function. This register function is used as a decorator to register\n",
    "# our module. \n",
    "HEAD_REGISTRY = Registry()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description = \"Description of your program\")\n",
    "parser.add_argument('directory', help=\"path to the input file\")\n",
    "args = parser.parse_args()\n",
    "file_path = args.directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stickler_thief(arr: list):\n",
    "    sub_array = []\n",
    "    check_indices = []\n",
    "\n",
    "    while arr:\n",
    "        max_element = max(arr)\n",
    "        max_indices = [ind for ind, ele in enumerate(arr) if ele = max_element]\n",
    "        if len(max_indices) > 0 :\n",
    "            max_indices = max_indices.sort()\n",
    "            if any([(max_indices[i]-max_indices[i+1])==1 for i in range(len(max_indices))]):\n",
    "                check_indices.extend([[indices-1, indices+1] for indices in max_indices])\n",
    "                check_val = [arr[i] for i in check_indices]\n",
    "                check_val = min(check_val)\n",
    "                min_ind = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "               \n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial on Pytorch gather\n",
    "# torch.gather(input, dim, index, *, sparse_grad=False, out=None) â†’ Tensor\n",
    "# --> dim = the dimension along which gather occurs\n",
    "# --> index = tensor specifying the gathering rules\n",
    "\n",
    "import torch\n",
    "M = torch.tensor([[1,2,3], [4,7,9], [19,9,23]]) \n",
    "indexes = torch.tensor([1,1,2])\n",
    "indexes = indexes.unsqueeze(0)\n",
    "print(indexes.shape)\n",
    "out = M.gather(dim=0,index=indexes)\n",
    "\n",
    "### Understanding pytorch scatter_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# dummy variable for checking\n",
    "x = torch.rand(64,3,32,32)\n",
    "mal_num = 6\n",
    "fixed = torch.zeros_like(x.clone()[:-mal_num], requires_grad=False)\n",
    "adv = (torch.zeros_like(x.clone()[-mal_num:])- x[-mal_num:] + 127.5/255).requires_grad_(True)\n",
    "adv_pad = torch.cat((fixed,adv), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# dummy variables \n",
    "d_batch = torch.rand(64,3,32,32)\n",
    "d_idx = torch.tensor([11,17,6,5,31,51])\n",
    "adv_pad = torch.zeros_like(x.clone(),requires_grad=False)\n",
    "\n",
    "def denorm(self, data_batch, mu, std):\n",
    "        \"\"\"\n",
    "        Convert a batch of tensors to their original scale.\n",
    "        Args:\n",
    "            batch (torch.Tensor): Batch of normalized tensors.\n",
    "            mean (torch.Tensor or list): Mean used for normalization.\n",
    "            std (torch.Tensor or list): Standard deviation used for normalization.\n",
    "        Returns:\n",
    "            torch.Tensor: batch of tensors without normalization applied to them.\n",
    "        \"\"\"\n",
    "        if isinstance(mean, list):\n",
    "            mean = torch.tensor(mu).to(self.device)\n",
    "        if isinstance(std, list):\n",
    "            std = torch.tensor(std).to(self.device)\n",
    "        return torch.clamp(data_batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1), 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution \n",
    "import torch\n",
    "B, H, W = 2, 5, 2\n",
    "x = torch.arange(B*H*W).view(B,H,W)\n",
    "print(x)\n",
    "idx = torch.randperm(x.size(1))\n",
    "print(idx)\n",
    "y = x[:,idx]\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Here's a simple CNN and loss function:\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "def loss_fn(predictions, targets):\n",
    "    return F.nll_loss(predictions, targets)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "num_models = 10\n",
    "batch_size = 64\n",
    "data = torch.randn(batch_size, 1, 28, 28, device=device)\n",
    "\n",
    "targets = torch.randint(10, (64,), device=device)\n",
    "\n",
    "model = SimpleCNN().to(device=device)\n",
    "predictions = model(data)  # move the entire mini-batch through the model\n",
    "\n",
    "loss = loss_fn(predictions, targets)\n",
    "loss.backward()  # back propagate the 'average' gradient of this mini-batch\n",
    "\n",
    "def compute_grad(sample, target):\n",
    "    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n",
    "    target = target.unsqueeze(0)\n",
    "\n",
    "    prediction = model(sample)\n",
    "    loss = loss_fn(prediction, target)\n",
    "\n",
    "    return torch.autograd.grad(loss, list(model.parameters()))\n",
    "\n",
    "\n",
    "def compute_sample_grads(data, targets):\n",
    "    \"\"\" manually process each sample with per sample gradient \"\"\"\n",
    "    sample_grads = [compute_grad(data[i], targets[i]) for i in range(batch_size)]\n",
    "    sample_grads = zip(*sample_grads)\n",
    "    sample_grads = [torch.stack(shards) for shards in sample_grads]\n",
    "    return sample_grads\n",
    "\n",
    "per_sample_grads = compute_sample_grads(data, targets)\n",
    "print(per_sample_grads[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call, vmap, grad\n",
    "\n",
    "params = {k: v.detach() for k, v in model.named_parameters()}\n",
    "buffers = {k: v.detach() for k, v in model.named_buffers()}\n",
    "\n",
    "# params = {k: v.detach() for k, v in model.conv2.named_parameters()}\n",
    "# buffers = {k: v.detach() for k, v in model.conv2.named_buffers()}\n",
    "\n",
    "def compute_loss(params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    predictions = functional_call(model, (params, buffers), (batch,))\n",
    "    loss = loss_fn(predictions, targets)\n",
    "    return loss\n",
    "ft_compute_grad = grad(compute_loss)\n",
    "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n",
    "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, data, targets)\n",
    "print(ft_per_sample_grads['conv2.weight'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "b=128\n",
    "portion=0.2\n",
    "mal_num = math.ceil(b*portion)\n",
    "print(type(mal_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
